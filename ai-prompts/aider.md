I am thinking of setting up two ollama servers on the same desktop which has 4 GPUs. One on the smaller older gpus and one on the two newer gpus. Build  a proxy server to select the server based on model size. The older GPUs can process models up to 8GB. Ideally I want to use the better server even for smaller models unless its busy. The proxy should also support multiple users and require a bearer token for security. There should be a rake command to generate a new user. There should be proper logging stored in /var/logging/ollama-proxy. Im happy to use SQLlite if needed. Must use ruby, or even rails if it would make implementation easier. I want this to run as a service under the ollama user. That way it can use the existing ollama repo of models between both servers. Each server should have a configurable port, and the proxy can use the default Ollama port by default but have a config to change its port too. I want some basic tests, but the whole implementation should ideally be as simple as possible but production ready. Im going to create a new repo for it. Use ai-prompts/claude-initial-prompt.md for how to set this up. Make sure to use Ruby 3.4.4 and Rails 8.0.2.

Run generator commands and make token optimisations to limit cost and improve results
